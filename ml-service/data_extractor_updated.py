# data_extractor_updated.py
"""
Extractor de datos actualizado para los 13 edificios modulares
Usa datos de MongoDB y archivos GeoJSON modulares
"""

import pymongo
import pandas as pd
import json
from datetime import datetime, timedelta
from pathlib import Path
import os
from dotenv import load_dotenv

load_dotenv()

# Los 13 edificios seleccionados
SELECTED_BUILDINGS = [
    'E-12', 'E-13', 'E-14', 'E-16', 'E-18', 
    'E-19', 'E-20', 'E-21', 'E-23', 'E-25', 
    'E-26', 'E-27', 'E-27-B'
]

def connect_to_mongodb():
    """Conectar a MongoDB"""
    try:
        mongo_uri = os.getenv('MONGO_URI')
        if not mongo_uri:
            raise ValueError('MONGO_URI no est√° definido en .env')
        
        client = pymongo.MongoClient(mongo_uri)
        db = client.get_database()
        client.admin.command('ping')
        print('‚úÖ Conectado a MongoDB')
        return db, client
    except Exception as e:
        print(f'‚ùå Error conectando a MongoDB: {e}')
        raise

def load_geo_data():
    """Cargar datos geogr√°ficos desde archivos modulares"""
    try:
        geo_path = Path(__file__).parent.parent.parent / 'project' / 'assets' / 'geo'
        
        with open(geo_path / 'Edificios.json', 'r', encoding='utf-8') as f:
            edificios = json.load(f)
        
        with open(geo_path / 'Caminos.json', 'r', encoding='utf-8') as f:
            caminos = json.load(f)
        
        with open(geo_path / 'Entradas.json', 'r', encoding='utf-8') as f:
            entradas = json.load(f)
        
        print('‚úÖ Archivos GeoJSON cargados')
        print(f'   Edificios: {len(edificios["features"])}')
        print(f'   Caminos: {len(caminos["features"])}')
        print(f'   Entradas: {len(entradas["features"])}')
        
        return edificios, caminos, entradas
    except Exception as e:
        print(f'‚ö†Ô∏è  Error cargando archivos GeoJSON: {e}')
        return None, None, None

def extract_building_data_from_mongo():
    """Extraer datos de edificios desde MongoDB (solo los 13 seleccionados)"""
    db, client = connect_to_mongodb()
    
    try:
        # Obtener edificios
        buildings = list(db.buildings.find({
            '_id': {'$in': SELECTED_BUILDINGS}
        }))
        
        # Obtener anal√≠ticas de edificios
        analytics = list(db.building_analytics.find({
            'buildingId': {'$in': SELECTED_BUILDINGS}
        }))
        
        print(f'‚úÖ Edificios extra√≠dos: {len(buildings)}')
        print(f'‚úÖ Anal√≠ticas extra√≠das: {len(analytics)}')
        
        return buildings, analytics
    finally:
        client.close()

def extract_event_data(days_back=90):
    """
    Extraer datos de eventos para entrenamiento
    Solo eventos asociados a los 13 edificios
    """
    db, client = connect_to_mongodb()
    
    try:
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        # Obtener anal√≠ticas de eventos asociados a los 13 edificios
        event_analytics = list(db.event_analytics.find({
            'date': {'$gte': cutoff_date},
            'buildingId': {'$in': SELECTED_BUILDINGS}
        }))
        
        print(f'‚úÖ Anal√≠ticas de eventos extra√≠das: {len(event_analytics)}')
        
        # Preparar datos
        data = []
        for analytics in event_analytics:
            event_date = analytics.get('date')
            if isinstance(event_date, str):
                event_date = datetime.fromisoformat(event_date.replace('Z', '+00:00'))
            
            if event_date:
                day_of_week = event_date.weekday()
                hour = event_date.hour
            else:
                day_of_week = 0
                hour = 12
            
            row = {
                'viewCount': analytics.get('viewCount', 0),
                'uniqueVisitors': analytics.get('uniqueVisitors', 0),
                'dayOfWeek': day_of_week,
                'hour': hour,
                'category_count': len(analytics.get('category', [])),
                'popularityScore': analytics.get('popularityScore', 0),
                'attendance': analytics.get('actualAttendance') or analytics.get('attendancePrediction') or analytics.get('uniqueVisitors', 0)
            }
            
            data.append(row)
        
        df = pd.DataFrame(data)
        
        # Guardar datos extra√≠dos
        data_dir = Path(__file__).parent / 'data'
        data_dir.mkdir(exist_ok=True)
        csv_path = data_dir / f'event_data_{datetime.now().strftime("%Y%m%d")}.csv'
        df.to_csv(csv_path, index=False)
        
        print(f'‚úÖ Datos guardados en {csv_path}')
        print(f'üìä Total de registros: {len(df)}')
        
        return df
    finally:
        client.close()

def extract_mobility_data(days_back=90):
    """
    Extraer datos de movilidad para los 13 edificios
    """
    db, client = connect_to_mongodb()
    
    try:
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        # Obtener anal√≠ticas solo de los 13 edificios
        building_analytics = list(db.building_analytics.find({
            'date': {'$gte': cutoff_date},
            'buildingId': {'$in': SELECTED_BUILDINGS}
        }))
        
        print(f'‚úÖ Anal√≠ticas de movilidad extra√≠das: {len(building_analytics)}')
        
        data = []
        for analytics in building_analytics:
            # Calcular hora pico
            peak_hours = analytics.get('peakHours', [])
            peak_hour = 12  # Default
            if peak_hours and len(peak_hours) > 0:
                max_peak = max(peak_hours, key=lambda x: x.get('count', 0))
                peak_hour = max_peak.get('hour', 12)
            
            # Contar eventos en ese edificio ese d√≠a
            date = analytics.get('date')
            building_id = analytics.get('buildingId')
            
            events_count = 0
            if date and building_id:
                events_count = db.events.count_documents({
                    'building': building_id,
                    'date': {
                        '$gte': date,
                        '$lt': datetime.combine(date, datetime.max.time())
                    }
                })
            
            # Calcular demanda basada en m√©tricas
            view_count = analytics.get('viewCount', 0)
            unique_visitors = analytics.get('uniqueVisitors', 0)
            score = (view_count * 0.4) + (unique_visitors * 0.3) + (events_count * 10)
            
            if score > 100:
                demand = 'Alta'
            elif score > 50:
                demand = 'Media'
            else:
                demand = 'Baja'
            
            row = {
                'buildingId': building_id,
                'viewCount': view_count,
                'uniqueVisitors': unique_visitors,
                'dayOfWeek': analytics.get('date').weekday() if analytics.get('date') else 0,
                'hour': 12,  # Usar mediod√≠a como referencia
                'peakHour': peak_hour,
                'eventsCount': events_count,
                'averageViewDuration': analytics.get('averageViewDuration', 0),
                'mobility_demand': demand
            }
            
            data.append(row)
        
        df = pd.DataFrame(data)
        
        # Guardar datos
        data_dir = Path(__file__).parent / 'data'
        data_dir.mkdir(exist_ok=True)
        csv_path = data_dir / f'mobility_data_{datetime.now().strftime("%Y%m%d")}.csv'
        df.to_csv(csv_path, index=False)
        
        print(f'‚úÖ Datos de movilidad guardados en {csv_path}')
        print(f'üìä Total de registros: {len(df)}')
        
        return df
    finally:
        client.close()

def extract_saturation_data(days_back=90):
    """
    Extraer datos de saturaci√≥n para los 13 edificios
    """
    db, client = connect_to_mongodb()
    
    try:
        cutoff_date = datetime.now() - timedelta(days=days_back)
        
        # Anal√≠ticas de edificios
        building_analytics = list(db.building_analytics.find({
            'date': {'$gte': cutoff_date},
            'buildingId': {'$in': SELECTED_BUILDINGS}
        }))
        
        print(f'‚úÖ Anal√≠ticas para saturaci√≥n extra√≠das: {len(building_analytics)}')
        
        data = []
        for analytics in building_analytics:
            # Calcular suma de peak visits
            peak_hours = analytics.get('peakHours', [])
            peak_visits = sum(ph.get('count', 0) for ph in peak_hours)
            
            view_count = analytics.get('viewCount', 0)
            unique_visitors = analytics.get('uniqueVisitors', 0)
            
            # Calcular nivel de saturaci√≥n
            score = (view_count * 0.3) + (unique_visitors * 0.2) + (peak_visits * 0.5)
            
            if score > 150:
                saturation = 3  # Alta
            elif score > 100:
                saturation = 2  # Media
            elif score > 50:
                saturation = 1  # Baja
            else:
                saturation = 0  # Normal
            
            row = {
                'buildingId': analytics.get('buildingId'),
                'viewCount': view_count,
                'uniqueVisitors': unique_visitors,
                'dayOfWeek': analytics.get('date').weekday() if analytics.get('date') else 0,
                'hour': 12,
                'peakVisits': peak_visits,
                'averageViewDuration': analytics.get('averageViewDuration', 0),
                'popularityScore': 0,  # No aplica para edificios
                'type': 0,  # 0 = Edificio
                'saturationLevel': saturation
            }
            
            data.append(row)
        
        df = pd.DataFrame(data)
        
        # Guardar datos
        data_dir = Path(__file__).parent / 'data'
        data_dir.mkdir(exist_ok=True)
        csv_path = data_dir / f'saturation_data_{datetime.now().strftime("%Y%m%d")}.csv'
        df.to_csv(csv_path, index=False)
        
        print(f'‚úÖ Datos de saturaci√≥n guardados en {csv_path}')
        print(f'üìä Total de registros: {len(df)}')
        print(f'üìà Distribuci√≥n de saturaci√≥n:')
        print(df['saturationLevel'].value_counts())
        
        return df
    finally:
        client.close()

def verify_data_quality():
    """Verificar la calidad de los datos extra√≠dos"""
    print('\nüîç VERIFICACI√ìN DE CALIDAD DE DATOS')
    print('=' * 60)
    
    db, client = connect_to_mongodb()
    
    try:
        # Verificar edificios
        buildings_count = db.buildings.count_documents({
            '_id': {'$in': SELECTED_BUILDINGS}
        })
        print(f'‚úÖ Edificios en BD: {buildings_count}/13')
        
        # Verificar anal√≠ticas
        analytics_count = db.building_analytics.count_documents({
            'buildingId': {'$in': SELECTED_BUILDINGS}
        })
        print(f'‚úÖ Anal√≠ticas de edificios: {analytics_count}')
        
        # Verificar eventos
        events_count = db.events.count_documents({
            'building': {'$in': SELECTED_BUILDINGS}
        })
        print(f'‚úÖ Eventos asociados: {events_count}')
        
        # Verificar anal√≠ticas de eventos
        event_analytics_count = db.event_analytics.count_documents({
            'buildingId': {'$in': SELECTED_BUILDINGS}
        })
        print(f'‚úÖ Anal√≠ticas de eventos: {event_analytics_count}')
        
        # Verificar peakHours
        with_peak_hours = db.building_analytics.count_documents({
            'buildingId': {'$in': SELECTED_BUILDINGS},
            'peakHours': {'$exists': True, '$ne': []}
        })
        print(f'‚úÖ Anal√≠ticas con peakHours: {with_peak_hours}')
        
        # Recomendaciones
        print('\nüìù Recomendaciones:')
        if buildings_count < 13:
            print('   ‚ö†Ô∏è  Ejecuta: npm run load-buildings-modular')
        if analytics_count < 100:
            print('   ‚ö†Ô∏è  Ejecuta: npm run generate-fake-data')
        if events_count < 5:
            print('   ‚ö†Ô∏è  Ejecuta: npm run generate-events')
        
        if buildings_count == 13 and analytics_count > 100:
            print('   ‚úÖ Los datos est√°n listos para entrenamiento ML')
        
    finally:
        client.close()

if __name__ == '__main__':
    print('üîÑ EXTRACTOR DE DATOS ML - 13 EDIFICIOS MODULARES')
    print('=' * 60)
    print()
    
    # Verificar calidad de datos
    verify_data_quality()
    
    print('\n' + '=' * 60)
    print('üìä EXTRAYENDO DATOS PARA MODELOS ML')
    print('=' * 60)
    print()
    
    # Extraer datos para cada modelo
    print('1Ô∏è‚É£ Extrayendo datos de eventos...')
    event_df = extract_event_data(days_back=90)
    print(f'   ‚úÖ {len(event_df)} registros extra√≠dos\n')
    
    print('2Ô∏è‚É£ Extrayendo datos de movilidad...')
    mobility_df = extract_mobility_data(days_back=90)
    print(f'   ‚úÖ {len(mobility_df)} registros extra√≠dos\n')
    
    print('3Ô∏è‚É£ Extrayendo datos de saturaci√≥n...')
    saturation_df = extract_saturation_data(days_back=90)
    print(f'   ‚úÖ {len(saturation_df)} registros extra√≠dos\n')
    
    print('=' * 60)
    print('üéâ EXTRACCI√ìN COMPLETADA')
    print('=' * 60)
    print()
    print('üìù Siguiente paso: Entrenar modelos con:')
    print('   python train_all_models.py')
    print()

